{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_x, batch_y = mnist.train.next_batch(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 784)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(batch_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 필요한 함수들 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(self):\n",
    "        result = 1.0 / (1.0 + np.exp(-self.x))\n",
    "        self.back = result*(1-result)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tanh(self):\n",
    "    result = (np.exp(self.x)-np.exp(-self.x))/(np.exp(self.x)+np.exp(-self.x))\n",
    "    self.back = 1-result^2\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0,x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def backward_relu(x):\n",
    "    return np.maximum(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    max = np.max(x,axis=1).reshape(-1,1) # 오버플로우 방지\n",
    "    temp = np.exp(x-max)\n",
    "    return temp / np.sum(temp, axis=1).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def backward_softmax(x):\n",
    "     return logits-batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax_cross_entropy(logits,y):\n",
    "    return -np.mean(y*np.log(logits+1e-7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 하이퍼 파라미터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = 0.1\n",
    "num_steps = 500\n",
    "batch_size = 128\n",
    "display_step = 100\n",
    "n_hidden_1 = 256 \n",
    "n_hidden_2 = 256 \n",
    "num_input = 784 \n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 layers 신경망 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(129, 784)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.concatenate((np.ones((1,784)),batch_x)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class NeuralNet(Activation):\n",
    "\n",
    "    def __init__(self, x, num_input, n_hidden_1, n_hidden_2, num_classes, y):\n",
    "        self.params={} # W에 bias 포함\n",
    "        self.params['W1'] = np.random.randn(num_input+1, n_hidden_1) \n",
    "        self.params['W2'] = np.random.randn(n_hidden_1+1, n_hidden_2)\n",
    "        self.params['W3'] = np.random.randn(n_hidden_2+1, num_classes)\n",
    "        self.x = np.concatenate((np.ones((1,784)),x))\n",
    "        self.y = y\n",
    "#        self.back = 1\n",
    "    \n",
    "    def forward(self):\n",
    "        W1,W2,W3 = self.params['W1'],self.params['W2'],self.params['W3']\n",
    "        \n",
    "        self.layer_1 = relu(np.matmul(self.x, W1))\n",
    "        self.layer_2 = relu(np.matmul(self.layer_1, W2))\n",
    "        self.output = softmax(np.matmul(self.layer_2, W3))l\n",
    "\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, epochs, lr):\n",
    "        W1,W2,W3 = self.params['W1'],self.params['W2'],self.params['W3']\n",
    "\n",
    "        for k in range(epochs):\n",
    "#            for i in range(self.x.shape[0]):\n",
    "            dW3 = np.multiply(self.softmax(np.matmul(self.layer_2,W3)).back, y-W3) \n",
    "            dW2 = np.multiply(self.relu(np.matmul(self.layer_1,W2)).back, np.dot(W2, dW3))\n",
    "            dW1 = np.multiply(self.relu(np.matmul(self.x,W1)).back, np.dot(W1, dW2))                \n",
    "                \n",
    "            self.params[\"W1\"] = -lr*dW1\n",
    "            self.params[\"W2\"] = -lr*dW2\n",
    "            self.params[\"W3\"] = -lr*dW3\n",
    "                \n",
    "      \n",
    "    \n",
    "    def relu(self):\n",
    "        self.back = np.maximum(0,1)\n",
    "        return np.maximum(self.x,0)\n",
    " \n",
    "    def sigmoid(self):\n",
    "        k = 1.0 / (1.0 + np.exp(-self.x))\n",
    "        self.back = k*(1-k)\n",
    "        return k\n",
    "       \n",
    "    def tanh(self):\n",
    "        k = (np.exp(self.x)-np.exp(-self.x))/(np.exp(self.x)+np.exp(-self.x))\n",
    "        self.back = 1-(k**2)\n",
    "        return k\n",
    "    \n",
    "    def softmax(x):\n",
    "        max = np.max(x,axis=1).reshape(-1,1) # 오버플로우 방지\n",
    "        temp = np.exp(x-max)\n",
    "        k = temp / np.sum(temp, axis=1).reshape(-1,1)\n",
    "        self.back = 1-k\n",
    "        return k\n",
    "    \n",
    "    def loss(self):\n",
    "        logits = self.output #self.forward(self.x)\n",
    "        return softmax_cross_entropy(logits, self.y)\n",
    "        \n",
    "    def accuracy(self):\n",
    "        yhat = self.output #self.forward(self.x)\n",
    "        yhat = np.argmax(yhat, axis=1)\n",
    "        y = np.argmax(self.y, axis=1)\n",
    "        return np.sum(y==yhat)/float(self.x.shape[0])\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "net = NeuralNet(num_input, n_hidden_1, n_hidden_2, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.00000000e+000,   0.00000000e+000,   0.00000000e+000, ...,\n",
       "          0.00000000e+000,   1.00000000e+000,   0.00000000e+000],\n",
       "       [  0.00000000e+000,   0.00000000e+000,   0.00000000e+000, ...,\n",
       "          0.00000000e+000,   1.00000000e+000,   0.00000000e+000],\n",
       "       [  0.00000000e+000,   4.17315385e-311,   7.63049099e-050, ...,\n",
       "          0.00000000e+000,   1.10926040e-176,   0.00000000e+000],\n",
       "       ..., \n",
       "       [  0.00000000e+000,   0.00000000e+000,   1.86221425e-108, ...,\n",
       "          0.00000000e+000,   3.89830047e-051,   0.00000000e+000],\n",
       "       [  0.00000000e+000,   0.00000000e+000,   0.00000000e+000, ...,\n",
       "          0.00000000e+000,   1.00000000e+000,   0.00000000e+000],\n",
       "       [  0.00000000e+000,   3.53701596e-320,   0.00000000e+000, ...,\n",
       "          0.00000000e+000,   1.00000000e+000,   0.00000000e+000]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.predict(batch_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4355178925223713"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.loss(batch_x, batch_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200704"
      ]
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W1.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Activation:\n",
    "\n",
    "    def __init__(self, x):\n",
    "        self.x = x\n",
    "        self.back = 1\n",
    "\n",
    "    def sigmoid(self):\n",
    "        output = 1.0 / (1.0 + np.exp(-self.x))\n",
    "        self.back = output*(1-output)\n",
    "        return output\n",
    "    \n",
    "    def relu(self):\n",
    "        self.back = np.maximum(0,1)\n",
    "        return np.maximum(self.x,0)\n",
    "    \n",
    "    def tanh(self):\n",
    "        output = (np.exp(self.x)-np.exp(-self.x))/(np.exp(self.x)+np.exp(-self.x))\n",
    "        self.back = 1-(output**2)\n",
    "        return output\n",
    "    \n",
    "    def softmax(x):\n",
    "        max = np.max(x,axis=1).reshape(-1,1) # 오버플로우 방지\n",
    "        temp = np.exp(x-max)\n",
    "        output = temp / np.sum(temp, axis=1).reshape(-1,1)\n",
    "        self.back = 1-output\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4 # 0.0001\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        idx = it.multi_index\n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = float(tmp_val) + h\n",
    "        fxh1 = f(x) # f(x+h)\n",
    "        \n",
    "        x[idx] = tmp_val - h \n",
    "        fxh2 = f(x) # f(x-h)\n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        \n",
    "        x[idx] = tmp_val # 値を元に戻す\n",
    "        it.iternext()   \n",
    "        \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(0)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.zeros_like(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork(object):\n",
    "\n",
    "\tdef __init__(self, inputs, hidden, outputs, activation='tanh', output_act='softmax'):\n",
    "\t\t\n",
    "\t\t# Hidden layer activation function\n",
    "\t\tif activation == 'sigmoid':\n",
    "\t\t\tself.activation = sigmoid\n",
    "\t\t\tself.activation_prime = sigmoid_prime\n",
    "\t\telif activation == 'tanh':\n",
    "\t\t\tself.activation = tanh\n",
    "\t\t\tself.activation_prime = tanh_prime\n",
    "\t\telif activation == 'linear':\n",
    "\t\t\tself.activation = linear\n",
    "\t\t\tself.activation_prime = linear_prime\n",
    "\n",
    "\t\t# Output layer activation function\n",
    "\t\tif output_act == 'sigmoid':\n",
    "\t\t\tself.output_act = sigmoid\n",
    "\t\t\tself.output_act_prime = sigmoid_prime\n",
    "\t\telif output_act == 'tanh':\n",
    "\t\t\tself.output_act = tanh\n",
    "\t\t\tself.output_act_prime = tanh_prime\n",
    "\t\telif output_act == 'linear':\n",
    "\t\t\tself.output_act = linear\n",
    "\t\t\tself.output_act_prime = linear_prime\n",
    "\t\telif output_act == 'softmax':\n",
    "\t\t\tself.output_act = softmax\n",
    "\t\t\tself.output_act_prime = linear_prime\n",
    "\n",
    "\t\t# Weights initializarion\n",
    "\t\tself.wi = np.random.randn(inputs, hidden)/np.sqrt(inputs)\n",
    "\t\tself.wo = np.random.randn(hidden + 1, outputs)/np.sqrt(hidden)\n",
    "\n",
    "\t\t# Weights updates initialization\n",
    "\t\tself.updatei = 0\n",
    "\t\tself.updateo = 0\n",
    "\n",
    "\n",
    "\tdef feedforward(self, X):\n",
    "\n",
    "\t\t# Hidden layer activation\n",
    "\t\tah = self.activation(np.dot(X, self.wi))\n",
    "\t\t\t\n",
    "\t\t# Adding bias to the hidden layer results\n",
    "\t\tah = np.concatenate((np.ones(1).T, np.array(ah)))\n",
    "\n",
    "\t\t# Outputs\n",
    "\t\ty = self.output_act(np.dot(ah, self.wo))\n",
    "\n",
    "\t\t# Return the results\n",
    "\t\treturn y\n",
    "\n",
    "\n",
    "\tdef fit(self, X, y, epochs=10, learning_rate=0.2, learning_rate_decay = 0 , momentum = 0, verbose = 0):\n",
    "\t\t\n",
    "\t\t# Timer start\n",
    "\t\tstartTime = time.time()\n",
    "\n",
    "\t\t# Epochs loop\n",
    "\t\tfor k in range(epochs):\n",
    "\t\n",
    "\t\t\t# Dataset loop\n",
    "\t\t\tfor i in range(X.shape[0]):\n",
    "\n",
    "\t\t\t\t# Hidden layer activation\n",
    "\t\t\t\tah = self.activation(np.dot(X[i], self.wi))\n",
    "\t\t\t\n",
    "\t\t\t\t# Adding bias to the hidden layer\n",
    "\t\t\t\tah = np.concatenate((np.ones(1).T, np.array(ah))) \n",
    "\n",
    "\t\t\t\t# Output activation\n",
    "\t\t\t\tao = self.output_act(np.dot(ah, self.wo))\n",
    "\n",
    "\t\t\t\t# Deltas\t\n",
    "\t\t\t\tdeltao = np.multiply(self.output_act_prime(ao),y[i] - ao)\n",
    "\t\t\t\tdeltai = np.multiply(self.activation_prime(ah),np.dot(self.wo, deltao))\n",
    "\n",
    "\t\t\t\t# Weights update with momentum\n",
    "\t\t\t\tself.updateo = momentum*self.updateo + np.multiply(learning_rate, np.outer(ah,deltao))\n",
    "\t\t\t\tself.updatei = momentum*self.updatei + np.multiply(learning_rate, np.outer(X[i],deltai[1:]))\n",
    "\n",
    "\t\t\t\t# Weights update\n",
    "\t\t\t\tself.wo += self.updateo\n",
    "\t\t\t\tself.wi += self.updatei\n",
    "\n",
    "\t\t\t# Print training status\n",
    "\t\t\tif verbose == 1:\n",
    "\t\t\t\tprint 'EPOCH: {0:4d}/{1:4d}\\t\\tLearning rate: {2:4f}\\t\\tElapse time [seconds]: {3:5f}'.format(k,epochs,learning_rate, time.time() - startTime)\n",
    "\t\t\t\t\n",
    "\t\t\t# Learning rate update\n",
    "\t\t\tlearning_rate = learning_rate * (1 - learning_rate_decay)\n",
    "\n",
    "\tdef predict(self, X): \n",
    "\n",
    "\t\t# Allocate memory for the outputs\n",
    "\t\ty = np.zeros([X.shape[0],self.wo.shape[1]])\n",
    "\n",
    "\t\t# Loop the inputs\n",
    "\t\tfor i in range(0,X.shape[0]):\n",
    "\n",
    "\t\t\ty[i] = self.feedforward(X[i])\n",
    "\n",
    "\t\t# Return the results\n",
    "\t\treturn y\n",
    "\n",
    "\n",
    "# Activation functions\n",
    "def sigmoid(x):\n",
    "\treturn 1.0/(1.0 + np.exp(-x))\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "\treturn sigmoid(x)*(1.0-sigmoid(x))\n",
    "\n",
    "def tanh(x):\n",
    "\treturn np.tanh(x)\n",
    "\n",
    "def tanh_prime(x):\n",
    "\treturn 1.0 - x**2\n",
    "\n",
    "def softmax(x):\n",
    "    return (np.exp(np.array(x)) / np.sum(np.exp(np.array(x))))\n",
    "\n",
    "def softmax_prime(x):\n",
    "    return softmax(x)*(1.0-softmax(x))\n",
    "\n",
    "def linear(x):\n",
    "\treturn x\n",
    "\n",
    "def linear_prime(x):\n",
    "\treturn 1"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
