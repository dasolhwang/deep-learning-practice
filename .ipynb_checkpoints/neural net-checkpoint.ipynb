{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_x, batch_y = mnist.train.next_batch(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 784)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(batch_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 필요한 함수들 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(self):\n",
    "        result = 1.0 / (1.0 + np.exp(-self.x))\n",
    "        self.back = result*(1-result)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tanh(self):\n",
    "    result = (np.exp(self.x)-np.exp(-self.x))/(np.exp(self.x)+np.exp(-self.x))\n",
    "    self.back = 1-result^2\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0,x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def backward_relu(x):\n",
    "    return np.maximum(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    max = np.max(x,axis=1).reshape(-1,1) # 오버플로우 방지\n",
    "    temp = np.exp(x-max)\n",
    "    return temp / np.sum(temp, axis=1).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def backward_softmax(x):\n",
    "     return logits-batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax_cross_entropy(logits,y):\n",
    "    return -np.mean(y*np.log(logits+1e-7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 하이퍼 파라미터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = 0.1\n",
    "num_steps = 500\n",
    "batch_size = 128\n",
    "display_step = 100\n",
    "n_hidden_1 = 256 \n",
    "n_hidden_2 = 256 \n",
    "num_input = 784 \n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 layers 신경망 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(129, 784)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.concatenate((np.ones((1,784)),batch_x)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(Activation):\n",
    "\n",
    "    def __init__(self, x, num_input, n_hidden_1, n_hidden_2, num_classes, y):\n",
    "        self.params={} # W에 bias 포함\n",
    "        self.params['W1'] = np.random.randn(num_input+1, n_hidden_1) \n",
    "        self.params['W2'] = np.random.randn(n_hidden_1+1, n_hidden_2)\n",
    "        self.params['W3'] = np.random.randn(n_hidden_2+1, num_classes)\n",
    "        self.x = np.concatenate((np.ones((1,784)),x))\n",
    "        self.y = y\n",
    "#        self.back = 1\n",
    "    \n",
    "    def forward(self):\n",
    "        W1,W2,W3 = self.params['W1'],self.params['W2'],self.params['W3']\n",
    "        \n",
    "        self.layer_1 = relu(np.matmul(self.x, W1))\n",
    "        self.layer_2 = relu(np.matmul(self.layer_1, W2))\n",
    "        self.output = softmax(np.matmul(self.layer_2, W3))l\n",
    "\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, epochs, lr):\n",
    "        W1,W2,W3 = self.params['W1'],self.params['W2'],self.params['W3']\n",
    "\n",
    "        for k in range(epochs):\n",
    "#            for i in range(self.x.shape[0]):\n",
    "            dW3 = np.multiply(self.softmax(np.matmul(self.layer_2,W3)).back, y-W3) \n",
    "            dW2 = np.multiply(self.relu(np.matmul(self.layer_1,W2)).back, np.dot(W2, dW3))\n",
    "            dW1 = np.multiply(self.relu(np.matmul(self.x,W1)).back, np.dot(W1, dW2))                \n",
    "                \n",
    "            self.params[\"W1\"] = -lr*dW1\n",
    "            self.params[\"W2\"] = -lr*dW2\n",
    "            self.params[\"W3\"] = -lr*dW3\n",
    "                \n",
    "      \n",
    "    \n",
    "    def relu(self):\n",
    "        self.back = np.maximum(0,1)\n",
    "        return np.maximum(self.x,0)\n",
    " \n",
    "    def sigmoid(self):\n",
    "        k = 1.0 / (1.0 + np.exp(-self.x))\n",
    "        self.back = k*(1-k)\n",
    "        return k\n",
    "       \n",
    "    def tanh(self):\n",
    "        k = (np.exp(self.x)-np.exp(-self.x))/(np.exp(self.x)+np.exp(-self.x))\n",
    "        self.back = 1-(k**2)\n",
    "        return k\n",
    "    \n",
    "    def softmax(x):\n",
    "        max = np.max(x,axis=1).reshape(-1,1) # 오버플로우 방지\n",
    "        temp = np.exp(x-max)\n",
    "        k = temp / np.sum(temp, axis=1).reshape(-1,1)\n",
    "        self.back = 1-k\n",
    "        return k\n",
    "    \n",
    "    def loss(self):\n",
    "        logits = self.output #self.forward(self.x)\n",
    "        return softmax_cross_entropy(logits, self.y)\n",
    "        \n",
    "    def accuracy(self):\n",
    "        yhat = self.output #self.forward(self.x)\n",
    "        yhat = np.argmax(yhat, axis=1)\n",
    "        y = np.argmax(self.y, axis=1)\n",
    "        return np.sum(y==yhat)/float(self.x.shape[0])\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = NeuralNet(num_input, n_hidden_1, n_hidden_2, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.00000000e+000,   0.00000000e+000,   0.00000000e+000, ...,\n",
       "          0.00000000e+000,   1.00000000e+000,   0.00000000e+000],\n",
       "       [  0.00000000e+000,   0.00000000e+000,   0.00000000e+000, ...,\n",
       "          0.00000000e+000,   1.00000000e+000,   0.00000000e+000],\n",
       "       [  0.00000000e+000,   4.17315385e-311,   7.63049099e-050, ...,\n",
       "          0.00000000e+000,   1.10926040e-176,   0.00000000e+000],\n",
       "       ..., \n",
       "       [  0.00000000e+000,   0.00000000e+000,   1.86221425e-108, ...,\n",
       "          0.00000000e+000,   3.89830047e-051,   0.00000000e+000],\n",
       "       [  0.00000000e+000,   0.00000000e+000,   0.00000000e+000, ...,\n",
       "          0.00000000e+000,   1.00000000e+000,   0.00000000e+000],\n",
       "       [  0.00000000e+000,   3.53701596e-320,   0.00000000e+000, ...,\n",
       "          0.00000000e+000,   1.00000000e+000,   0.00000000e+000]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.predict(batch_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4355178925223713"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.loss(batch_x, batch_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200704"
      ]
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W1.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Activation:\n",
    "\n",
    "    def __init__(self, x):\n",
    "        self.x = x\n",
    "        self.back = 1\n",
    "\n",
    "    def sigmoid(self):\n",
    "        output = 1.0 / (1.0 + np.exp(-self.x))\n",
    "        self.back = output*(1-output)\n",
    "        return output\n",
    "    \n",
    "    def relu(self):\n",
    "        self.back = np.maximum(0,1)\n",
    "        return np.maximum(self.x,0)\n",
    "    \n",
    "    def tanh(self):\n",
    "        output = (np.exp(self.x)-np.exp(-self.x))/(np.exp(self.x)+np.exp(-self.x))\n",
    "        self.back = 1-(output**2)\n",
    "        return output\n",
    "    \n",
    "    def softmax(x):\n",
    "        max = np.max(x,axis=1).reshape(-1,1) # 오버플로우 방지\n",
    "        temp = np.exp(x-max)\n",
    "        output = temp / np.sum(temp, axis=1).reshape(-1,1)\n",
    "        self.back = 1-output\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4 # 0.0001\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        idx = it.multi_index\n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = float(tmp_val) + h\n",
    "        fxh1 = f(x) # f(x+h)\n",
    "        \n",
    "        x[idx] = tmp_val - h \n",
    "        fxh2 = f(x) # f(x-h)\n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        \n",
    "        x[idx] = tmp_val # 値を元に戻す\n",
    "        it.iternext()   \n",
    "        \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(0)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.zeros_like(10)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
